{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Modern CNNs\n",
    "\n",
    "## <u>강의 내용 정리</u>\n",
    "\n",
    "### Brief History of CNN on ImageNet (until 2015)\n",
    "\n",
    "Image Net 대회에서 큰 성과를 얻은 모델들 \n",
    "\n",
    "* **AlexNet** ( Krizhevsky, 2012 )\n",
    "\n",
    "  ![alexnet](https://cdn-images-1.medium.com/max/1600/0*xPOQ3btZ9rQO23LK.png)\n",
    "\n",
    "  - First use of Relu -> to overcome Gredient vanishing \n",
    "  - Used normalization layers \n",
    "  - Data Augmetation\n",
    "  - Dropout(0.5)\n",
    "  - Batch size = 128 \n",
    "  - SGD Momentum(0.9)\n",
    "  - 0.01 learning rate, reduced by 10\n",
    "  - L2 weight decay 5e-4\n",
    "  - emsemble of 7 CNNs (18.2 -> 15.4)\n",
    "\n",
    "* **ZFNet** (Zeiler and Fergus, 2013) -> AlexNet 의 구조에서 몇가지 변화\n",
    "\n",
    "  ![ZFnet](https://image.slidesharecdn.com/artifaciaaimeet-10dec-161220080555/95/introduction-to-cnn-with-application-to-object-recognition-35-638.jpg?cb=1482222133)\n",
    "\n",
    "* **VGGNet** (Simonyan and Zisserman, 2014) \n",
    "\n",
    "  ![vggNet](https://cdn-images-1.medium.com/max/1600/0*V1muWIDnPVwZUuEv.png)\n",
    "\n",
    "  - ILSBRC`14; 2등, localization ; 1등 (찾으려는 부분이 어디에 있는지 까지 알려주는 것 )\n",
    "  - Small filters, Deeper Networks(more Non-linearity)\n",
    "    - 16 ~ 19layers (VGG16, VGG19)\n",
    "    - 3x3 conv layers with 1 stride(3x3을 쌓는 것과 7x7을 쌓는것의 효과는 같았다. ), pad 1 & 2x2 maxpool with 2 stride\n",
    "    - 7.3% top 5 error (previously ; 11.7%)\n",
    "    - Fewer Parameters : 2 x (3x3xc) vs (5x5xc)  -> Regularization effect \n",
    "\n",
    "* GoogLeNet \n",
    "\n",
    "  ![googlenet](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/GoogleNet.png)\n",
    "\n",
    "  - Deeper with computational efficiency \n",
    "\n",
    "  - 22 layers\n",
    "\n",
    "  - Efficient **Inception modul** ; Network In Network \n",
    "\n",
    "    ![inception module](https://hackathonprojects.files.wordpress.com/2016/09/inception_implement.png?w=649&h=337)\n",
    "\n",
    "    * Inception -v3 ; Factorization of Filters\n",
    "\n",
    "      ![Inception v3](https://cburke.me/ssu_preserves/images/08_transfer_learning_flowchart.png)\n",
    "\n",
    "  - Global average pooling attempted\n",
    "\n",
    "  - only 5million parameters -> Fully connected layer가 상당히 줄었다. \n",
    "\n",
    "  - ILSVRC`14 1등 (6.7% top 5 error)\n",
    "\n",
    "* ResNet -> 끝판왕!\n",
    "\n",
    "  - ILSVRC`15(3.6% top 5 error; 인간 보다 낫다. ), COCO 2015 등 1위를 휩쓸었다. \n",
    "\n",
    "  - **Full ResNet architecture**\n",
    "\n",
    "    ![Resnet](https://cdn-images-1.medium.com/max/1600/0*pkrso8DZa0m6IAcJ.png)\n",
    "\n",
    "    - Stack residual block\n",
    "    - every residual block has two 3x3 conv layers\n",
    "    - Peridodically double # of filters and downsample spatially using stride 2\n",
    "    - Additional conv layer at the beginning \n",
    "    - No FC layers at the end(only FC1000 to output classes)\n",
    "\n",
    "  - **Bottleneck Architecture**\n",
    "\n",
    "    for deeper networkds(ResNet-50+), use 'bottleneck' layer to improve efficiency(similar to GoogLeNet) - Deep 한 Network 의 한계가 극복되었다.(Gredient는 잘 전달되고, Emsemble효과를 얻을 수 있다. )\n",
    "\n",
    "    ![bottleneck](https://i.stack.imgur.com/kbiIG.png)\n",
    "\n",
    "    - Inception -ResNet\n",
    "\n",
    "      ![inception resnet](https://image.slidesharecdn.com/04-170118145519/95/vc-37-638.jpg?cb=1484751708)\n",
    "\n",
    "* **An analysis of deep neural network models for practical applications 2017**\n",
    "\n",
    "  ![](https://pbs.twimg.com/media/C6u_ugFWsAI3XgZ.jpg)\n",
    "\n",
    "  - size of bubble : 메모리 사용량 \n",
    "\n",
    "### Recent Papers \n",
    "\n",
    "; have two stream. One is 'more performance', and the other is 'more efficient' (모바일, 디바이스 단에서 분석 시도)\n",
    "\n",
    "#### DenseNet (Densely Connected Convolutional Networks, 2017, 08)\n",
    "\n",
    "; 채널을 Concatenation (뚱뚱하게 만든다. )\n",
    "\n",
    "[자세한 내용은 Paper에서 참조](http://www.cs.cornell.edu/~gaohuang/papers/DenseNet-CVPR-Slides.pdf)\n",
    "\n",
    "- Dense Connectivity \n",
    "\n",
    "  ![](https://i.imgur.com/vsqXLzE.png)\n",
    "\n",
    "- Forward Propagation ; 앞으로 가면서, 채널(c)이 Concat 된다. \n",
    "\n",
    "- Composite Layer in DenseNet with Bottleneck Layer \n",
    "\n",
    "  - 뚱뚱한 것을 줄이기 위해 시도 된다.\n",
    "  - 1xk -> batch normalization -> ReLU-> Conv1 -> 4xk channels -> BM -> ReLU ->Conv3x3 -> k chanels \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1920/1*_Y7-f9GpV7F93siM1js0cg.jpeg)\n",
    "\n",
    "- - Dense block안에서만 Concatenation connectivity 존재\n",
    "\n",
    "**장점**\n",
    "\n",
    "- Strong Gradient flow \n",
    "- Parameter and Computational Efficiency \n",
    "- Maintains Low Cmplexity features\n",
    "\n",
    "#### SqueezeNet (2016 , 11)\n",
    "\n",
    " -  For better efficiency (AlexNet 정도의 성능을 냄)\n",
    "\n",
    " -  Replace 3x3 filters with 1x1 \n",
    "\n",
    "- 인풋 채널 수를 3x3으로 줄임 -> for total ; (input channels) x (number of filter) x (3x3)\n",
    "\n",
    "  $\\rightarrow$ it`s 'Bottle Neck '\n",
    "\n",
    "- Downsample late in the network so that convolution layers have large activation maps\n",
    "\n",
    "  large activation maps (due t0 delayed downsampling) can lead to higher classification accuracy \n",
    "\n",
    "**Architecture**\n",
    "\n",
    "![](https://image.slidesharecdn.com/2016-05-25-deep-learning-03-160531032409/95/recent-developments-in-deep-learning-47-638.jpg?cb=1464665084)\n",
    "\n",
    "* pruning : '0'에 가까운 weight를 날리고, accuracy가 회복되지 않을 때까지, retraining을 반복한다.\n",
    "* **details**\n",
    "  - 1-pixel border of zero-padding in the input data to 3x3 filters\n",
    "  - ReLU is applied \n",
    "  - Dropout ratio of 50% after the fire9 Module \n",
    "  - Lack of fully-connected layers\n",
    "  - initial learning rate : 0.04\n",
    "  - implemented on **Caffe**\n",
    "* 성능 - SqueezeNet + Simple Bypass 구조가 가장 나은 성능을 낸다. \n",
    "\n",
    "#### Xception (keras 만든 사람, 2017)\n",
    "\n",
    "; MobileNet 과 idea가 같다. \n",
    "\n",
    "; [논문 링크](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "\n",
    "큰 모델은 많은 수의 파라미터를 의미 하고 이는 overfitting을 야기하고 많은 컴퓨터 리소스를 필요로 한다. inception으로 부터, mapping cross channel correlation and spatial correlation, 이 두가지를 하나의 convolution kernel로 수행할 수 있을 것이다. ( by inception module, these two clrrelations are sufficiently decoupled)\n",
    "\n",
    "![](http://openresearch.ai/uploads/default/original/1X/cc4a46e88db02065db5b9db5de97eb13223d4404.png)\n",
    "\n",
    "- 1X1 conv 가 먼저 들어갔다는 것은 채널전체를 먼저 다본다는 것, 다음 3x3으로 spartial 한 방향으로 보는 것(1x1이 먼저 인지 3x3이 먼저 인지는 상관이 없다. )\n",
    "\n",
    "- Depthwise Separable Convolution \n",
    "\n",
    "  > Commonly clalled \"separable convolution\" in deep learning frameworks such as TF, Keras; a sparial convolution performed independently over each channel of an input followed by a pointwise convolution \n",
    "\n",
    "  - In term of the Operations. The presence of absence of non-linearity after the first operation\n",
    "\n",
    "  Regular convolution  -> **Inception** -> Depthwise separable convolution\n",
    "\n",
    "  :: Xception Hypothesis : make the mapping that entirely decouples the cross-channels correlations and sparial  correlations\n",
    "\n",
    "#### MobileNet \n",
    "\n",
    "#### ( Efficient Convolutional Neural Networks for Mobile vision applications)\n",
    "\n",
    "[paper`s link](https://arxiv.org/pdf/1704.04861.pdf); MobileNet can be applied to various recognition tasks for efficient on device intelligence\n",
    "\n",
    ":: 모든 논문에서 가장 첫번째 layer는 안 건드리는게 불문률이다. -> it affects the performance severely\n",
    "\n",
    "- Depthwie Separable Convolution \n",
    "\n",
    "  채널 별로 다른 weight를 준다. ( Depthwise conv + Pointwise Convolution(1x1 conv))\n",
    "\n",
    "  -> and, Reduction in computations (if we use 3x3 depthwise Conv, we get between 8 to 9 times less computations)\n",
    "\n",
    "- Width Multiplier & Resolution Multiplier (reducing input-size)\n",
    "\n",
    "  - For a given layer and width multiplier $\\alpha$, the number of input channels $M$ becomes $\\alpha M$ and the number of output channels $N$  becomes $\\alpha N$ - where $\\alpha$ with typical settings of 1, 0.5, 0.6 and 0.25\n",
    "\n",
    "  - The second hyper-parameter to reducce the computational cost of a neural network is a resolution multiplier $\\rho$\n",
    "\n",
    "  - Computational cost : $D_k \\times D_k \\times \\alpha D_F \\times \\rho D_F + \\alpha M \\times \\alpha N \\times \\rho D_F \\times \\rho D_F$\n",
    "\n",
    "    -> 실험에서는 Depth가 큰게 더 좋은 성능을 냈다. \n",
    "\n",
    "#### ShuffleNet (: an Extremely efficient convolutional neural network for Mobile devices, from China) \n",
    "\n",
    "; 의식의 흐름은 따라가는 논문이라는 평 ; [paper`s link](https://arxiv.org/pdf/1707.01083.pdf)\n",
    "\n",
    "- Main ideas \n",
    "\n",
    "  - Use depthwise separable convolution \n",
    "  - Grouped convolution of 1x1 convolution layers - pointwise group convolution \n",
    "  - Channnel shuffle operation after pointwise group convolution (Grouped convolution)\n",
    "\n",
    "- Grouped Convolution of AlexNext \n",
    "\n",
    "  AlexNet`s primary motivation was to allow the training of the network over two Nvidia GTX580 opus with 1.5 GB of memory each \n",
    "\n",
    "  AlexNet without filter groups is not only less efficient( voth in parameters and compute ), but also slightly less accurate!\n",
    "\n",
    "- Grouped Convolution\n",
    "\n",
    "  ![](https://blog.yani.io/assets/images/posts/2017-08-10-filter-group-tutorial/convlayer.svg)\n",
    "\n",
    "  Acovolutional lyaer with 2 filter groups. Note that each of the filters in the grouped convolutional layer is now exactly half the depth, i.e , half the parameters and half the compute as the original filter. \n",
    "\n",
    "- ResNeXt \n",
    "\n",
    "  Equivalent building blocks of ResNext(cardinality = 32)\n",
    "\n",
    "- 1X1 Grouped Convolution with channel shuffling (Not randomly )\n",
    "\n",
    "  - ex> 1, 2, 3, 4, 5, 6, 7, 8, 9 -> 1, 4, 7  2, 5, 8  3, 6, 9 \n",
    "  - if multiple group convolutions stack together, there is one side effect -> Output from a certain channel are only derived from a small fraction of input channels \n",
    "  - If we allow group convolution to obtain input data from diffferent groups, the input and output channels will be fully related\n",
    "\n",
    "- Channel shuffle operation \n",
    "\n",
    "  ```python\n",
    "  def channel_shuffle(name, x, num_groups):\n",
    "      with tf.variable_scope(name) as scope:\n",
    "          n, h, w, c = x.shape.as_list()\n",
    "          x_reshaped = tf.reshape(x, [-1, h, w, num_groups, c // num_groups])\n",
    "          x_transposed = tf.transpose(x_transposed, [-1, h, w, c])\n",
    "          output = tf.reshape(x_transposed, [-1, h, w, c])\n",
    "          return output \n",
    "  ```\n",
    "\n",
    "- ShuffleNet Units\n",
    "\n",
    "   ![](https://image.slidesharecdn.com/pr12shufflenet-171211145836/95/shufflenet-pr054-18-638.jpg?cb=1513004776)\n",
    "\n",
    "  -> 가로세로를 1/2 하면 채널은 2배로 늘리는게 관습이다. \n",
    "\n",
    "- Complexity \n",
    "\n",
    "  for example, given the input size cxhxw and the bottleneck channel m \n",
    "\n",
    "  ResNet - hw(2cm + 9m^2)\n",
    "\n",
    "  ResNext - hw(2cm + 9m^2/g)\n",
    "\n",
    "  ShuffleNet - hw(2cm/g + 9m)  -> 연산량이 비교적 매우 작다. \n",
    "\n",
    "#### ResNeXt (aggregated Residual Transformations for deep neural Networks)\n",
    "\n",
    "; [paper`s link](https://arxiv.org/pdf/1611.05431.pdf)\n",
    "\n",
    "VGGNet - stacking building blocks of the same shape -> ResNeXt - Stacking  modules of the same topology \n",
    "\n",
    "; unlike VGGNet, the family of Inception models have demonstrated that carefully designed topolgies are able to achieve compelling accuracy  [ ; important common property is split-transrom-merge strategy ; Split = 1x1conv, transform-3x3, 5x5 conv merge - concatenation ]\n",
    "\n",
    "- Inception learns ResNet \n",
    "\n",
    "- Resnet learns Incpeiton (REsNext ; uniform multi-branch)\n",
    "\n",
    "  Concatenation and addition are interchangeable, uniform multi branching can be done by group-conv\n",
    "\n",
    "- Model coapacity (# of parameters )\n",
    "\n",
    "  original ResNet - 256x64 + 3x3x64x64 + 64x256 = 70k\n",
    "\n",
    "  REsNeXt (right - with bottleneck width d and cardinality C(그룹의 개수)) \n",
    "\n",
    "  = Cx(256xd + 3x3xdxd + dx256) = 70km when C = 32 and d = 4\n",
    "\n",
    "#### NASNet (; Learning transferable architectures for scalable Image Recognition)\n",
    "\n",
    "; [Paper`s link](https://arxiv.org/pdf/1707.07012.pdf) , [Tensorflow Pre-trained model](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet), Auto ml conducted\n",
    "\n",
    "이제는 구조까지 컴퓨터가 찾는다. \n",
    "\n",
    "- **Neural architecture search** \n",
    "- Motivation & Idea\n",
    "  - NAS used 800GPUs for 28days resulting in 22,400 GPU-hours (ImageNet dataset) \n",
    "  - Many State-of-the-art models have repeated modules \n",
    "  - Searching for a good architecture on the far smaller CIFAR-10 dataset, and automatically transfer the learned architecture to ImageNet \n",
    "  - Achieving this transferabliity by designing a search space so that the complexity of the architecture is independent of the depth of the network \n",
    "  - All convolutional networks in search space are composed of convolutional lyaers(or 'cells')with identical structure but different weights \n",
    "- Method\n",
    "  - overall architectures of the convolutional nets are manually predetermined (앞에는 사람이 도와줌)\n",
    "    - Normal cell - convolutional cells that return a feature map of the same dimesion \n",
    "    - Reduction cell - convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two \n",
    "  - Using common heuristic to double the number of filters in the output whenever the spatial acitvation size is reduced \n",
    "- Model and Algorithms \n",
    "  1. Select a hidden state from $h_i, h_{i-1} or from the set of hidden states created in previous blocks.\n",
    "  2. Select a second hidden state from the same options as in '1'\n",
    "  3. Select an operation to apply to the hidden  state selected in '1'\n",
    "  4. Select an operation to apply to the hidden state selected in '2'\n",
    "  5. Select a method to combine the outputs of '3' and '4' to create a new hidden state.\n",
    "\n",
    "##### **Best Architecture ; NASNet - A, B, C**\n",
    "\n",
    "- NASNet - A\n",
    "\n",
    "![A](https://user-images.githubusercontent.com/5595332/34201683-44cc3648-e5b0-11e7-98ea-734a8eff6161.png)\n",
    "\n",
    "![C](https://i.ytimg.com/vi/fbCcJaSQPPA/maxresdefault.jpg)\n",
    "\n",
    "#### ENAS (2018, 02 )\n",
    "\n",
    "- Motivation \n",
    "\n",
    "  - NAS used 800GPUs for 28days and NASNet Used 450GPUs for 3~4 days \n",
    "  - Meanwhile, using less resources tends to produce less compelling result\n",
    "  - Computational bottleneck of NAS is the training of each child model to convergence, only to measure its accureacy whilst throwing away all the trained weights \n",
    "  - 찾는 속도를 빠르게 하여, 특정 data set에 잘 맞는 구조를 찾아내겠다. \n",
    "\n",
    "- Main idea \n",
    "\n",
    "  Forcing all child models to share weights  to eschew training each child model from scratch to convergence\n",
    "\n",
    "  -> Using Single GTX1080 GPU, the search for architectures takes less than 16 hours compared to NAS, reduction is more than  1000 times \n",
    "\n",
    "- Directed Acyclic Graph (**DAG**)\n",
    "\n",
    "  ![](http://openresearch.ai/uploads/default/original/1X/9892db8975abcd36291d866f626923b454a953e2.png)\n",
    "\n",
    "  - ENAS`s DAG is the superposition of all possible child models in a search space of NAS, where the nodes represent the local computations and the edges represent the flow of information \n",
    "  - The local computations at each node have their own parameters, which are used onlly when the paricular computation is acitvated \n",
    "  - Therefore, ENAS`s design allows parameters to be shared among all child models \n",
    "\n",
    "- Seacch spaces for CNN \n",
    "\n",
    "  - the **6** operations available for controller are \n",
    "    - convolution with filter sizeds 3x3 and 5x5\n",
    "    - Depthwise-separable convolutions with filter 3x3 and 5x5 \n",
    "    - Max pooling and average pooling of kernel size 3x3\n",
    "  - Making the describe set of decisions for a total of L times, we can sample a network of L layers\n",
    "  - Since all decisions are independent, there are $6^L \\times 2^{L(L-1)/2}$\n",
    "  - When L=12, resulting in 1.6x10^29 possible networks \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
