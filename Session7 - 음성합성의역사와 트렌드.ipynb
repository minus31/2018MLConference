{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7 - 음성합성 역사와 최신 트렌드\n",
    "\n",
    "------\n",
    "\n",
    "## <u>강의 내용 정리</u>\n",
    "\n",
    "### Overview of Text-to-Speech System \n",
    "\n",
    "; 응용분야로는 시각장애인 보조 기계, Home speaker, 자동차의 Navigation 등이 있다. \n",
    "\n",
    "음성합성은 사람을 연구하는 것으로 시작되었다. 사람의 말에서 Voice와 Unvoice(노이즈)를 구별하게끔 특징값을 추출하여, 주파수분석, Spectrogram 분석등이 행해졌다. **기본적으로 음성 신호는 Time-varying 신호여서 어떻게 temporal variation을 모델링하는냐가 매우 중요하다.** \n",
    "\n",
    "위의 언급처럼 전통적인 접근방식은 1. 음성데이터는 정제되기전, 매우 지저분하기 때문에 음성의 특징을 잘 살려주는 요소를 어떻게 추출할 것 인가를 고민하고,  2. 두번째로 추출한 요소를 어떻게 학습할 것인가를 고민하는 두가지가 큰 줄기가 있었다. 최근 Deep learning이 적용되면서, Raw Data(지저분 한 채로)를 그냥 넣고, 하나의 모델이 앞단의 특징값을 추출하고, 이를 학습하는 것 까지 수행하는 End2End 학습방법이 적용되고 있다. \n",
    "\n",
    "### History \n",
    "\n",
    "##### 가장 간단한 Text to Speech, \n",
    "\n",
    "목소리를 저장 했다가 각 각 필요한 상황에 알맞는 소리를 틀어준다. \n",
    "\n",
    "​\t\t\t\t\t\t-> 말의 경우의 수가 적은 경우 매우유용하고 음질이 가장 뛰어나다.\n",
    "\n",
    ", 그러나 문제가 복잡해지고 필요한 말의 개수가 많아지면, 작은 Device에서는 물리적으로 불가능하다. \n",
    "\n",
    "##### Unit Selection Synthesis System  \n",
    "\n",
    "녹음된 음성을 원하는 기준에 맞게 잘라서 압축하여 저장 ->  합성은 Cost(target cost : 입력텍스트와 저장된 음성간의 cost , Concatenation cost : 연결하려는 음성들간의 cost)\n",
    "\n",
    "장점 : 높은 품질의 합성음\n",
    "\n",
    "단점 : 큰사이즈의 모델(따라서 학습과정 합성과정에 많은 리소스가 필요)이 필요하고 연결부위의 어색함이 있음(작은 System에는 적용이 불가능하다.)\n",
    "\n",
    "#####  Statistical Parametric Speech Synthesis System \n",
    "\n",
    "음성신호 특징과 텍스트 특징을 학습하여 저장 후 텍스트의 특징에 맞는 데이터를 이용해 음성을 만든다. (Hidden Markov Based Text to Speech; HTS )\n",
    "\n",
    "```Database -> Extract Feature(MELP, MFCC, STRAIGHT) -> Model training(HMM(예전엔 거의 이것만 사용했다.), DNN, RNN ) -> Parameter estimation(Estimate parameter for waveform; STRAIGHT, Dynamic generation)```\n",
    "\n",
    "장점 : 매우 작은 모델, 시스템에서 사용가능 \n",
    "\n",
    "단점 : 음성 품질이 낮았다.\n",
    "\n",
    "##### Hidden Markov Model based TTS System \n",
    "\n",
    "![Hidden Markov Model based TTS System](https://image.slidesharecdn.com/htsslides-150329102731-conversion-gate01/95/fundamentals-and-recent-advancesin-hmmbased-speech-synthesis-basic-fundamentals-22-638.jpg?cb=1427624903)\n",
    "\n",
    "추출한 특징 정보를 HMM로 학습 -> Full Context HMM 모델을 Clustering(Decision based context clustering) \n",
    "\n",
    "한계점 : 학습에 label된 DB가 필요, 모델의 표현력(기계스러운 소리가 난다.)이 약함, Over-smoothing \n",
    "\n",
    "##### Deep Learning based TTS system\n",
    "\n",
    "HMM 모델의 한계를 극복하기 위한 초기의 Deep-learning 모델 - 기존의 틀은 유지 한채 HMM 모델 부분을 뉴럴넷으로 대체 했다. \n",
    "\n",
    "——> 여기 까지 시도에도, **Unit Selection Synthesis System의 성능이 제일 좋았다.** \n",
    "\n",
    "##### Hybrid TTS System - Unit Selection + Statistical Parametric\n",
    "\n",
    "USS(실제 사람목소리 같은 자연스러움) + SPSS (다양한 텍스트 입력에 대해 대응이 가능)\n",
    "\n",
    "접근방법 : SPSS모델을 이용해 파라미터를 생성, 생성된 파라미터와 가장 가까운 USS data를 탐색하고, 이것으로 Speech를 생성 \n",
    "\n",
    "성능 : 현재 상용화된 State of the art performance - 현재 높은 품질의 TTS는 거의 이 솔루션을 사용하고 있다. \n",
    "\n",
    "### Recent Work \n",
    "\n",
    "##### Toward End to End TTS Scenario \n",
    "\n",
    "- 기존 방법의 한계 \n",
    "\n",
    "  언어에 대한 사전지식이 필요 (각 언어마다 다른 방식이 적용되어야 하기 때문에)\n",
    "\n",
    "  모델에 대한 사전지식이 필요 (Model마다 다른 Feature extraction사용 등 여러 문제 )\n",
    "\n",
    "  Data 라벨링 작업 필요 \n",
    "\n",
    "- Motivation \n",
    "\n",
    "  언어에 대한 사전지식 없이 할 수 있을까 - 언어의 확장이 쉽게 \n",
    "\n",
    "  모델에 대한 사전지식 없이 할 수 있을까\n",
    "\n",
    "  노가다 없이 할 수 있을까\n",
    "\n",
    "  더 높은 품질을 얻을 수 있을까\n",
    "\n",
    "- End to End TTS System \n",
    "\n",
    "  Text 입력으로부터 Wavefrom 출력을 바로 학습 \n",
    "\n",
    "  중간에 존재하는 수많은 과정들을 한꺼번에 \n",
    "\n",
    "이를 실현하기 위한 4가지 논문 : WaveNet, Deep Voice, Tacotron, Deep Voice2\n",
    "\n",
    "### WaveNet (16, 09)\n",
    "\n",
    "from GOOGLE Deep mind , Sequential 생성을 TTS에 적용해보자 \n",
    "\n",
    "Raw audio를 만드는 모델 (기존의 raw audio를 spectrum 과 같은 특징을 추출하던 것을 raw audio있는 그대로를 만드는 방향 ), 합성은 16bit, 16k sampling -> 1초 길이의 audio를 만들기 위해 16000개의 sample을 만들어야 한다.  <- 1초에 4만개 생성해야 자연스런 소리가 난다. \n",
    "\n",
    "기존의 Causal convolutional layers (RNN과 유사; 깊어지면 학습이 안됨)을 Dilated causal convolutional layers(모든 sample을 한번씩 만 보는 것) 로 바꿨다. \n",
    "\n",
    "* Techniques \n",
    "\n",
    "  1. Softmax distribution \n",
    "\n",
    "  \t\t16-bit integer value (65,536개의 확률)\n",
    "\n",
    "  \t\t$\\mu$-law compading transfromation, 256 quantization \n",
    "\n",
    "  \t\t:: $f(x_t) = sign(x_t)\\dfrac{\\ln(1 + \\mu|x_t|)}{\\ln(1 + \\mu)}$\n",
    "\n",
    "  2. ReLU -> Gated activation unit\n",
    "\n",
    "     PixelCNN / LSTM의 것과 동일 한것 \n",
    "\n",
    "     $ z = \\tanh(W_{f, k} * \\text{x}) \\odot \\sigma(W_{g, k} * \\text{x})$\n",
    "\n",
    "  3. Residual, Skip connections \n",
    "\n",
    "     더 깊은 학습이 가능해 진다. \n",
    "\n",
    "* Conditional WaveNet \n",
    "\n",
    "  시그널(condition; speaker identity, Character from TTS and etc) + 무슨 말을 하라 = 얻고자하는 말 \n",
    "\n",
    "  - Global condition - $ z = \\tanh(W_{f, k * \\text{x}}+ V_{g, k}^T h) \\odot \\sigma(W_{g, k} * \\text{x} + V_{g, k}^T h)$\n",
    "\n",
    "  - Local condition -$ z = \\tanh(W_{f, k * \\text{x}}+ V_{g, k}* y) \\odot \\sigma(W_{g, k} * \\text{x} + V_{g, k} * y) :: y = f(h)$\n",
    "\n",
    "    성능은 좋았으나, 데이터가 그대로 들어가진 않았다. \n",
    "\n",
    "WaveNet으로 생성한 오디오파일을 들려주심 - 어느정도 의미를 가진 소리 처럼 들렸다. \n",
    "\n",
    "그러나 상용화하기엔 시간이 너무 오래 걸렸다. 0.02초의 소리를 생성하기위해 1초가 걸렸다. \n",
    "\n",
    "#### Deep Voice (17, 02) , real time Neural text to speech \n",
    "\n",
    "from Daidu, End to End 로 한번에 되는 것은 아니지만 TTS의 각 모듈을 모두 DNN으로 구성했다. \n",
    "\n",
    "**Grapheme to Phoneme Model** \n",
    "\n",
    "- Bidirectional(학습)/Unidirectional(합성) GRU를 사용했고, 각 언어의 전문가들이 필요없어졌다. \n",
    "\n",
    "- Segmentaion Model \n",
    "\n",
    "  어느 구간이 특정 음소에 해당하는지 라벨링을 해야한다. \n",
    "\n",
    "  (Method)\n",
    "\n",
    "  Connectionist temporal classification(CTC)를 이용했다.(어느 음소인지 분류하는 것, lable은 많아질수록 overfitting이 되서, Trainset에 대해서는 loss 가 잘 낮아짐)\n",
    "\n",
    "  Single phone이 아닌 두개의 Phoneme을 pair로 사용\n",
    "\n",
    "**Phoneme Duration and Fundamental Frequency Model** \n",
    "\n",
    "  두 모델의 특성이 비슷해서 합쳐서 학습 \n",
    "\n",
    "  기존의HMM으로 학습하던 부분을 GRU로 대체 \n",
    "\n",
    "$ L_n = 1. \\ |\\hat{t}_n -t_n| + 2. \\ \\lambda_1CE(\\hat{p}_n, p_n) + 3. \\ \\lambda_2\\sum_{t=0}^{T-1}|\\hat{FO}_{n, t} - FO_{n, t}| +4. \\  \\lambda_3 \\sum_{t = 0}^{T-2}|\\hat{FO}_{n, t+1} = \\hat{FO}_{n, t}|$\n",
    "\n",
    "1. Phoneme duration error\n",
    "2. Negative log likelihood of the probability that the phoneme is voiced \n",
    "3. Fundamental frequency error \n",
    "4. penelty term ( FO는 현재의 것만 고려하기 때문에 소리가 들쭉날쭉한 하다. 이를 극복하기 위해 추가 되었다.)\n",
    "\n",
    "* Audio Synthesis Model ( Variant of WaveNet) \n",
    "\n",
    "![Deepvoice audio synthesis model](http://slideplayer.com/slide/12115002/70/images/81/DeepVoice+Audio+Synthesis+Model.jpg)\n",
    "\n",
    "$\\rightarrow$ 모든 모듈을 DNN으로 구현, **모듈들은 따로따로 학습을 했다**. 성능 면에서 주목할 점은 **속도가 매우 향상**되었다는 것이다. \n",
    "\n",
    "\n",
    "\n",
    "### Tacotron (17, 03)\n",
    "\n",
    "구글에서 만든 최초의 End to End generative TTS model 이다. \n",
    "\n",
    "Machine Translation에 사용하던 구조를 가져와 음성처리에 맞게 변형했다. (음성합성도 기계번역의 일종이라고 생각했다. ) \n",
    "\n",
    "* **전체 구조** \n",
    "\n",
    "  - CBHG (NMT에서 사용되는 모듈)\n",
    "  - Attention \n",
    "  - RNN\n",
    "\n",
    "  Input: Character( one hot -> 256 dimension vector)  Output : Spectrogram  \n",
    "\n",
    "![tacotron](https://www.androidheadlines.com/wp-content/uploads/2017/04/TacoTron_1.jpg)\n",
    "\n",
    "- Pre-net \n",
    "\n",
    "  FC-256-ReLU $\\rightarrow$ Dropout(0.5) $\\rightarrow$ FC-128-ReLU $\\rightarrow$ Dropout(0.5)\n",
    "\n",
    "- CBHG (Encoder)\n",
    "\n",
    "  - Conv 1D bank : k=16, conv-k-128-ReLU (가변적으로 character의 특성을 알 수있다. 어떤건 가까이 봐도 되지만, 어떤건 넓게 봐야하기 때문)\n",
    "  - Maxpooling: stride = 1, width=2\n",
    "  - Conv 1D projections : conv-3-128-ReLU-conv-3-128-linear\n",
    "  - Highway net : 4 layers of FC-128-ReLU\n",
    "  - Bidirectional GRU : 128 cells \n",
    "\n",
    "- Attention \n",
    "\n",
    "  1-layer GRU(256cells)\n",
    "\n",
    "  $u_i^t = \\nu^T tanh(W_1^\\prime h_i  +W_2^\\prime d_t)$\n",
    "\n",
    "  $a_i^t = softmax(u_i^t) \\ \\rightarrow$ character vector의 길이 만큼의 vector 중요한것에 대한 확률이 높게 된다. \n",
    "\n",
    "  $d_t^\\prime = \\sum_{i=1}{T_A}a_i^t h_i$\n",
    "\n",
    "  -------------\n",
    "\n",
    "- Decoder pre-net \n",
    "\n",
    "  FC-256-ReLU $\\rightarrow$ Dropout(0.5) $\\rightarrow$ FC-128-ReLU $\\rightarrow$ Dropout(0.5)\n",
    "\n",
    "- Decoder RNN\n",
    "\n",
    "  2-layers residual GRU(256cells)\n",
    "\n",
    "- Predict r frames \n",
    "\n",
    "  80 band mel-scale spectrogram \n",
    "\n",
    "  r = 2 ~ 5 \n",
    "\n",
    "- Decoder CBHG\n",
    "\n",
    "  - Conv 1D bank : k=8, conv-k-128-ReLU\n",
    "  - Maxpooling: stride = 1, width=2\n",
    "  - Conv 1D projections : conv-3-128-ReLU-conv-3-80-linear\n",
    "  - Highway net : 4 layers of FC-128-ReLU\n",
    "  - Bidirectional GRU : 128 cells \n",
    "\n",
    "- Synthesize waveform \n",
    "\n",
    "  - Griggin-Lim algorithm(1984) -> not included in training step \n",
    "\n",
    "**Conclusion** \n",
    "\n",
    "- **Frame based, faster than auto-regressive model(WaveNet)**\n",
    "- NO need to hand engineering \n",
    "- Need simple text normalization (ex. 16 -> sixteen)\n",
    "- Replacing Griffin-Lim model can improve performance \n",
    "\n",
    "### Deep Voice 2 - Multi Speaker TTS\n",
    "\n",
    "Multi Speaker TTS\n",
    "\n",
    "- Embedding speaker identity \n",
    "- 화자별로 모델을 만들 필요없어졌다. \n",
    "\n",
    "WaveNet for vocoder , Deep Voice + $\\alpha$, tacotron + $\\alpha$\n",
    "\n",
    "**Main Architecture** (+ speaker`s identity)\n",
    "\n",
    "Segmentation Model / Duration Model / Frequency Model / Vocal Model \n",
    "\n",
    "![](https://i0.wp.com/www.aimechanic.com/wp-content/uploads/2017/05/Deep-Voice-2.png?w=541)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/7529838/33115847-d78ec1d8-cfa5-11e7-8270-f73de61c14bd.png)\n",
    "\n",
    "* Trainable speaker embedding \n",
    "\n",
    "  - Site specific speaker Embeddings\n",
    "  - Recurrent Initialization \n",
    "  - Input Augmentation \n",
    "  - Feature Gating \n",
    "\n",
    "* Segmentation model \n",
    "\n",
    "  - Esimate phoneme location \n",
    "\n",
    "  - Convolutional recurrent architecture with CTC\n",
    "\n",
    "  - BN, residual connection $\\leftarrow$ 이부분이 추가되었다. \n",
    "\n",
    "    Deepvoice 1 - $h^{(l)} = relu(W^l * h^{l-1} +b^l)$\n",
    "\n",
    "    Deepvoice 2 - $h^{(l)} = relu(h^{l-1} +BN(W^l * h^{l-1}))$\n",
    "\n",
    "    ​                        $h^{(l)} = relu(h^{l-1} +BN(W^l * h^{l-1})\\cdot g_s)$\n",
    "\n",
    "* Duration model \n",
    "\n",
    "  - Regression problem to Classification (Log scale로 변환)\n",
    "  - Phoneme duration is discretized into log-scaled buckets \n",
    "  - speaker embedding \n",
    "    - initialize GRU \n",
    "    - INput augmentation \n",
    "\n",
    "* Frequency model \n",
    "\n",
    "  - GRU + sigmoid nonlinearity \n",
    "  - predict probablilty of voiced signal \n",
    "  - Predict f0 -> F0 예측 & classifying voice or non-voice?\n",
    "\n",
    "* Vocal model \n",
    "\n",
    "  - based on WaveNet architecture \n",
    "  - Two layer bidirectional QRNN\n",
    "  - speaker embedding (input augmentation) \n",
    "\n",
    "* Conclusion \n",
    "\n",
    "  단일 화자 비교 : Deep voice 1, Tacotron(griffin-Lim), Tacotron(WaveNet)과 비교해 가장 우수한 성능\n",
    "\n",
    "  다중 화자 비교 : VCTK data set에서는 Ground truth 보다 높은 성적(gt: 99.7, 40layer WaveNet : 100), Audiobooks 에 대해서는 Ground truth에 약간 못미치지만 다른 모델보다는 우수한 성적\n",
    "\n",
    "#### Additional; DeepVoice 3(17, 10), Tacotron 2(17, 12)\n",
    "\n",
    "##### Deep Voice 3 - 이제는 완전히 End to End\n",
    "\n",
    "Fully convolutional attention based model \n",
    "\n",
    "Training time is 10 times faster than Deep Voice 2 \n",
    "\n",
    "Training on 800 hours speech data over 2000 speakers - 음성인식 DB 활용 (소리의 quality 가 낮다.)\n",
    "\n",
    "- CBHG 를 Convolution Block으로 대체 \n",
    "\n",
    "#### Tacotron 2 \n",
    "\n",
    "Character embedding to mel-scale spectrogram \n",
    "\n",
    "Modified WaveNet model - 실시간 WaveNet 합성 가능\n",
    "\n",
    "Use mel spectrogram as input to WaveNet instead of linguistic, duration, and FO features \n",
    "\n",
    "![](https://1.bp.blogspot.com/-gqIm8wC2rqg/WkXYnYKMJqI/AAAAAAAAMTI/rhnP-f0qJgkBlmjI0v7cTY8otFZY9fI2wCLcBGAs/s1600/tacotron%2B2.png)\n",
    "\n",
    "$\\rightarrow$ Source는 공개 되지 않았지만, 지금의 SOTA이다. 이제는 Ground Truth, 인간과의 비교가 되었다. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
